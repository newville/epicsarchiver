#!/usr/bin/env python
#
# main pvarch application

import sys, os, time, getopt
from threading import Thread

try:

    import EpicsArchiver
except:
    print('cannot import EpicsArchiver')
    sys.exit(1)

from EpicsArchiver import MasterDB, Cache, ArchiveMaster, Archiver, \
     tformat, add_pvfile, startstop, ConnectionPool

from EpicsArchiver.config import logdir, master_db, data_dir, webfile_prefix
from EpicsArchiver.util import SEC_DAY


cpool = ConnectionPool()

def show_usage():
    print("""    pvarch:   run and interact with Epics PV Archiver and MySQL process
    pvarch -h            shows this message.
    pvarch status        shows cache and archiving status, some recent statistics.
    pvarch check         print # of archivedsin past 10 minutes. Should be >1!


    pvarch start         start the archiving process
    pvarch stop          stop the archiving process
    pvarch restart       restart the archiving process
    pvarch next          restart with 'next run' of data archives
    pvarch setinfo       set the run information for the most recent run

    pvarch add_pv        add a PV to the cache and archive
    pvarch add_pvfile    read a file of PVs to add to the Archiver

    pvarch drop_pv       remove a PV from cahce and archive

    pvarch list          prints a list of recent data archives
    pvarch save          save a list of recent data archives
    pvarch clean         clean up temporary data files used for web plotting.

    pvarch cache start        start cache process (if it's not already running)
           cache stop         stop  cache process
           cache restart      restart cache process
           cache status   t   show # of PVs cached in past t seconds (default=60)
           cache activity t   show list of PVs cached in past t seconds (default=60)
    """)
    sys.exit()


def set_runinfo():
    master  = ArchiveMaster()
    for i in master.runs.select(where='1=1 order by start_time desc limit 2'):
        dbname = i['db']
        if dbname != master.arch_db:
            print('setting run information for ', dbname)
            master.set_runinfo(dbname)
            print('done.')
    master.close()


def next_archive():
    master  = ArchiveMaster()
    old_dbname  = master.arch_db
    next_db = master.make_nextdb()

    master.stop_archiver()
    master.set_currentDB(next_db)

    master.close()
    run_archive(action='start')

def save_archives(args):
    " save archives to gzipped ascii files"
    m  = MasterDB()
    for db in (m.arch_db, master_db):
        m.save_db(dbname=db)
        if db in args: args.remove(db)

    for db in args:
        m.save_db(dbname=db)
    m.close()

def clean_web_datafiles():
    """clean old files from web data / plot directory:
    files older than 2 days will be deleted"""
    now = time.time()
    nclean = 0
    for i in os.listdir(data_dir):
        fname = os.path.abspath(os.path.join(data_dir,i))
        if (os.path.isfile(fname) and i.startswith(webfile_prefix)):
            mtime = os.stat(fname)[8]
            if (now - mtime) > 2*SEC_DAY:
                os.unlink(fname)
                nclean = nclean+1
    print('removed %i files from %s' % (nclean,data_dir))

def list_archives():
    master  = MasterDB()
    for i in master.runs_report():
        print(i)
    master.close()

def run_archive(action='start'):

    if not os.path.exists(logdir):   os.mkdir(logdir)
    sout    = os.path.join(logdir, "pvarch.log")
    serr    = os.path.join(logdir, "pvarch.err")
    pidfile = os.path.join(logdir, "pvarch.pid")

    dbconn  = cpool.get()
    a = Archiver(dbconn=dbconn)
    if 'start'==action:
        pid,status = a.get_pidstatus()
        if (pid != 0 and status != 'offline'):
            ret = a.get_nchanged(minutes=2,limit=10)
            if ret >  5:
                print("Archive appears to be running... try 'restart'?")
                cpool.put(dbconn)
                return

    startstop(stdout =sout,stderr=serr, pidfile=pidfile,
              process_name='pvarch', action=action)

    dbconn  = cpool.get()
    a = Archiver(dbconn=dbconn)
    if 'stop'==action:
        a.set_pidstatus(pid=0,status='stopping')
        time.sleep(1.0)

    elif 'start'==action:
        a.mainloop()

def run_cache(action=None):
    # not that cache is best run in it's own thread!

    if not os.path.exists(logdir):
        os.mkdir(logdir)
    sout    = os.path.join(logdir, "cache.log")
    serr    = os.path.join(logdir, "cache.err")
    pidfile = os.path.join(logdir, "cache.pid")

    dbconn  = cpool.get()
    c = Cache(dbconn=dbconn, pidfile=pidfile)

    if 'start'==action:
        status = c.get_cache_status()
        if status == 'running':
            ret = c.cache.select(where="ts> %i order by ts" % (time.time()-30.0))
            if len(ret) >  5:
                print("Archive appears to be running... try 'restart'?")
                cpool.put(dbconn)
                return
    elif 'stop'==action:
        c.db.set_autocommit(1)
        c.set_cache_status('stopping')
        c.set_cache_pid(0)
        c.shutdown()
        if action == 'stop':
            return
        else:
            action = 'start'

    def cache_thread(fcn_action='start'):
        dbconn  = cpool.get()
        c = Cache(dbconn=dbconn)
        if 'stop'==fcn_action:
            c.db.set_autocommit(1)
            c.set_cache_status('stopping')
            c.set_cache_pid(0)
            time.sleep(0.50)
        else:
            c.mainloop()

    if action=='start' and False:
        cache_thread()
    else:
        startstop(stdout =sout,stderr=serr, pidfile=pidfile,
                  process_name='pvcache', action=action,
                  func=cache_thread, fcn_action=action)


def read_pid(name='cache'):
    pidfile = "%s/%s.pid" % (logdir,name)
    try:
        pf  = open(pidfile,'r')
        pid = int(pf.read().strip())
        pf.close()
    except IOError:
        pid = None
    return pid

def cache_status(action='activity', dt=60):
    rpid   = read_pid(name='cache')
    cache = Cache()
    pid   = cache.get_pid()
    ret   = cache.get_recent(dt=dt)
    status= cache.get_cache_status()
    if pid != rpid:
        status = status + ' (pid not found)'

    cache.close()
    msg = '%i changed PVs cached in past %i seconds.  PID=%i  Status=%s'
    if 'check' == action:
        print(len(ret))
    else:
        if 'activity' == action:
            for r in ret:
                stime = tformat(r['ts'],format="%H:%M:%S")
                print("  %s   %.29s = %s" % (stime, r['pvname']+' '*20,r['value']))
        print(msg % (len(ret), int(dt), pid, status))


def show_archive_status(minutes=10):
    m    = MasterDB(dbconn=None)
    stat = m.arch_report(minutes=minutes)
    for i in stat: print(i)
    nchanged = m.arch_nchanged(minutes=minutes)
    print("%i values archived in past %i minutes" % (nchanged, minutes))
    m.close()

def archived_values(minutes=10):
    m    = MasterDB(dbconn=None)
    nchanged = m.arch_nchanged(minutes=minutes)
    m.close()
    return nchanged


def drop_pv(pvname):
    dbconn = cpool.get()
    Archiver(dbconn=dbconn).drop_pv(pvname)
    MasterDB(dbconn=dbconn).drop_pv(pvname)

def main():
    opts, rawargs = getopt.getopt(sys.argv[1:], "h", ["help"])
    try:
        cmd = rawargs.pop(0)
    except IndexError:
        cmd = 'help'
    for (k,v) in opts:
        if k in ("-h", "--help"): cmd = 'help'

    args = []
    for arg in rawargs:
        if ',' in arg:
            for i in arg.split(','):
                if len(i)>0:  args.append(i.strip())
        elif len(arg)>0:
            args.append(arg)

    if   'help' == cmd:
        show_usage()

    elif 'status' == cmd:
        if read_pid(name='cache') is None:
            print("Cache is not running: start Cache with 'pvarch cache start'!")
        else:
            print('Cache is running')

        if read_pid(name='pvarch') is None:
            print("Archiver is not running: start with 'pvarch start'")
        else:
            show_archive_status()

    elif 'check' == cmd:
        print(archived_values())

    elif cmd in ('start','stop','restart'):
        if 'restart' == cmd:
            run_archive(action='stop')
            time.sleep(3)
            run_archive(action='start')
        else:
            run_archive(action=cmd)

    elif 'next' == cmd:
        next_archive()
        print("should now run 'pvarch setinfo'")
        # set_runinfo()

    elif 'save' == cmd:
        save_archives(args)

    elif 'clean' == cmd:
        clean_web_datafiles()

    elif 'list' == cmd:
        list_archives()

    elif 'setinfo' == cmd:
        set_runinfo()

    elif 'add_pv' == cmd:
        c    = Cache()
        for pv in args:
            c.add_pv(pv)
        if len(args)>1:
            c.set_allpairs(args)

        time.sleep(0.1)
        print("Wait for PV to get to cache...")

        c.process_requests()
        req_table= c.db.tables['requests']
        requests_pending = True
        while requests_pending:
            pending_requests  = req_table.select()
            requests_pending = len(pending_requests) > 2
            time.sleep(0.5)
        c.close()

    elif 'add_pvfile' == cmd:
        for pvfile in args:
            add_pvfile(pvfile)

    elif 'drop_pv' == cmd:
        for pv in args:
            drop_pv(pv)

    elif 'cache' == cmd:
        action = args.pop(0)

        if action not in ('start','stop','restart','status','activity','check'):
            print("'pvarch cache' needs one of start, stop, restart, status, check, activity")
            print("    Try 'pvarch -h' ")

        if action in ('status','check','activity'):
            dt = 60
            if len(args)>0: dt = args.pop(0)
            cache_status(action=action,dt=float(dt))
        elif action ==  'restart':
            run_cache(action='stop')
            time.sleep(3)
            run_cache(action='start')
        else:
            run_cache(action=action)

    else: print("pvarch  unknown command '%s'.    Try 'pvarch -h'" % cmd)

main()
